{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run default-imports.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(params):\n",
    "    \n",
    "    print(\"Running experiment with following parameters: \")\n",
    "    print(params)\n",
    "    \n",
    "    filenames = {'MIMIC' : \"~/cohorts/hs_mimic.csv\", 'SINAI' : \"~/cohorts/hs_sinai_preprocessed.csv\"}\n",
    "\n",
    "    ''' first load and split data '''\n",
    "    data = Load().execute(filename=filenames[params['cohort']])\n",
    "    train, test = Split().execute(data,test_size=params['test_size'])\n",
    "    train = Impute().execute(train.copy(), imputation_method=params['imputation_method'])\n",
    "    \n",
    "    if params.get('transform_method'):\n",
    "        train = Scale().execute(train.copy(), params['target'],transform_method=params['transform_method'])\n",
    "    \n",
    "    if params.get('sampling_method'):\n",
    "        train = Sample().execute(train.copy(), params['target'],sampling_method=params['sampling_method'])        \n",
    "    \n",
    "    ''' fit models '''\n",
    "    models = Train().execute(train, target=params['target'], algorithms=[params['algorithm']], optimize=params['optimize_mode'])\n",
    "\n",
    "    ''' evaluate models '''\n",
    "    test = Impute().execute(test.copy(), imputation_method=params['imputation_method'])\n",
    "    if params.get('transform_method'):\n",
    "        test = Scale().execute(test.copy(), params['target'],transform_method=params['transform_method'])\n",
    "    \n",
    "    results = Evaluate().execute(test, target=params['target'], models=models)\n",
    "    \n",
    "    ''' obtain performance metrics '''\n",
    "    performance = {}        \n",
    "    performance['discrimination'] = get_discrimination_metrics(**results[params['algorithm']])     \n",
    "    performance['calibration'] = get_calibration_metrics(results[params['algorithm']]['y_true'],results[params['algorithm']]['y_probs'])\n",
    "    performance['clinical_usefulness'] = get_clinical_usefulness_metrics(performance['discrimination'])\n",
    "    \n",
    "    ''' interpret explanations '''\n",
    "    explanations = {}     \n",
    "    weighted_explanations = {}\n",
    "    if params.get('explainers'):\n",
    "        explanations = Explain().execute(train, models=models, target=params['target'], explainers=params['explainers'], exp_kwargs={'test':test})        \n",
    "        weighted_explanations = get_weighted_explanations(explanations[params['algorithm']])\n",
    "        \n",
    "    ''' store results '''\n",
    "    experiment = {'parameters' : params, 'performance' : performance, 'explanations' : explanations, 'weighted_explanations' : weighted_explanations}\n",
    "    \n",
    "    return experiment \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
